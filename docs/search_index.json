[["index.html", "Python Cookbook for the Casual Dabbler Chapter 1 Welcome 1.1 Using this book 1.2 Additional resources 1.3 Limitations 1.4 Why write a python book using the R publishing language? 1.5 About the author", " Python Cookbook for the Casual Dabbler Charles Coverdale 2022-04-13 Chapter 1 Welcome G’day and welcome to Python cookbook for the casual dabbler. Some history: I use the R language a lot for work and for side projects. However I’ll be the first to admit - for some things, python is simply much better. Rather than storing my bits and bobs of python code in random files and repos, I’ve created this book to systematize key the key processes I use often, in one convenient location. 1.1 Using this book In each chapter I’ve written up the background, methodology and code for a separate piece of analysis. Most of this code will not be extraordinary to the seasoned Python aficionado. The vast majority can be found elsewhere if you dig around on stackexchange or read some of the many python programming blogs and books. However I find that in classic Pareto style ~20% of my code contributes to the vast majority of my work output. Having this on hand will hopefully be useful to both myself and others. 1.2 Additional resources The Python community is continually writing new books and package documentation with great worked examples. Some of my favourites are: Python Beginners Guide Python Data Science Handbook 1.3 Limitations I’ll be honest with you - there’s bound to be bugs galore in this. If you find one (along with spelling errors etc) please email me at charlesfcoverdale@gmail.com with the subject line ‘Python Cookbook for the Casual Dabbler.’ 1.4 Why write a python book using the R publishing language? It’s a fair question. The simplest answer is that the R Markdown and Bookdown languages are excellent - and far more conducive to longer form content compared to jupyter notebooks. There are some quirks however. For example, to get python to run within the RStudio IDE I need to load up each chapter with some code (written in R) that looks something like this: library(reticulate)use_condaenv(\"tf\")reticulate::py_install(\"numpy\")reticulate::py_install(\"geopy\") Interested in publishing your own book using R Markdown or Bookdown? There’s some excellent books on the topic, however I find this guide as by far the most useful. 1.5 About the author Charles Coverdale is an economist based in Melbourne, Australia. He is passionate about economics, climate science, and building talented teams. You can get in touch with Charles on twitter to hear more about his current projects. "],["making-beautiful-charts-in-python.html", "Chapter 2 Making beautiful charts in Python 2.1 Importing python packages 2.2 Importing and cleaning data 2.3 Creating a line chart 2.4 Bar charts 2.5 Stacked bar charts 2.6 Line charts 2.7 Scatter plot 2.8 Histogram 2.9 Multiple charts in single plot 2.10 Annotating charts", " Chapter 2 Making beautiful charts in Python This chapter contains the code for some of my most used charts and visualization techniques. 2.1 Importing python packages Let’s load in some libraries that we will use again and again when making charts. import matplotlib.pyplot as plt import matplotlib.dates as mdates import pandas as pd import numpy as np import statistics from scipy.stats import norm from matplotlib.ticker import EngFormatter, StrMethodFormatter 2.2 Importing and cleaning data Let’s start by importing data from a csv and making it usable. In this example, we’ll use the weather profile from 2019 in Melbourne, Australia. We’ll also create a new column for a rolling average of the temperature. #Note non-ascii character in csv will stuff up the import, so we add this term: encoding=&#39;unicode_escape&#39; # Note: The full file location is this: # /Users/charlescoverdale/Documents/2021/Python_code_projects/learning_journal_v0-1/MEL_weather_2019.csv # Import csv df_weather= pd.read_csv(&quot;MEL_weather_2019.csv&quot;,encoding=&#39;unicode_escape&#39;) # Create a single data column and bind to df df_weather[&#39;Date&#39;] = pd.to_datetime(df_weather[[&#39;Year&#39;, &#39;Month&#39;, &#39;Day&#39;]]) # Drop the original three field date columns df_weather = df_weather.drop(columns=[&#39;Year&#39;, &#39;Month&#39;, &#39;Day&#39;]) # Let&#39;s change the name of the solar exposure column df_weather = df_weather.rename({&#39;Daily global solar exposure (MJ/m*m)&#39;:&#39;Solar_exposure&#39;, &#39;Rainfall amount (millimetres)&#39;:&#39;Rainfall&#39;, &#39;Maximum temperature (°C)&#39;: &#39;Max_temp&#39;}, axis=1) #Add a rolling average df_weather[&#39;Rolling_avg&#39;] = df_weather[&#39;Max_temp&#39;].rolling(window=7).mean() df_weather.head() 2.3 Creating a line chart Now that the data is in a reasonable format (e.g. there is a simple to use ‘Date’ column), let’s go ahead and make a line chart. # Now let&#39;s plot maximum temperature on a line chart plt.plot(df_weather[&#39;Date&#39;], df_weather[&#39;Max_temp&#39;], label=&#39;Maximum temperature&#39;, color=&#39;blue&#39;, alpha=0.2, linewidth=1.0, marker=&#39;&#39;) plt.plot(df_weather[&#39;Date&#39;], df_weather[&#39;Rolling_avg&#39;], label=&#39;7-day moving average&#39;, color=&#39;red&#39;, linewidth=1.0, marker=&#39;&#39;) plt.title(&#39;Maximum temperature in Melbourne (2019)&#39;, fontsize=12) plt.xlabel(&#39;&#39;, fontsize=10) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b&#39;)) plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) #plt.margins(x=0) plt.ylabel(&#39;&#39;, fontsize=10) plt.gca().yaxis.set_major_formatter(StrMethodFormatter(u&quot;{x:.0f}°C&quot;)) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(True) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.tick_params( axis=&#39;x&#39;, # changes apply to the x-axis which=&#39;both&#39;, # both major and minor ticks are affected bottom=False, # ticks along the bottom edge are off top=False, # ticks along the top edge are off labelbottom=True) # labels along the bottom edge are off plt.tick_params( axis=&#39;y&#39;, # changes apply to the y-axis which=&#39;both&#39;, # both major and minor ticks are affected left=False, # ticks along the bottom edge are off right=False, # ticks along the top edge are off labelleft=True) # labels along the bottom edge are off plt.grid(False) plt.gca().yaxis.grid(True) plt.legend(fancybox=False, framealpha=1, shadow=False, borderpad=1) plt.savefig(&#39;weather_chart_save.png&#39;,dpi=300,bbox_inches=&#39;tight&#39;) plt.show() 2.4 Bar charts # Chart 1: Bar plot # Get data country = [&#39;USA&#39;, &#39;Canada&#39;, &#39;Germany&#39;, &#39;UK&#39;, &#39;France&#39;] GDP_per_capita = [45,40,38,16,10] # Create plot plt.bar(country, GDP_per_capita, width=0.8, align=&#39;center&#39;,color=&#39;blue&#39;, edgecolor = &#39;black&#39;) # Labels and titles plt.title(&#39;GDP per capita of select OECD countries&#39;) plt.xlabel(&#39;Test x label&#39;) plt.ylabel(&#39;&#39;) #A dd bar annotations to barchart # Location for the annotated text i = 1.0 j = 1.0 # Annotating the bar plot with the values (total death count) for i in range(len(country)): plt.annotate(GDP_per_capita[i], (-0.1 + i, GDP_per_capita[i] + j)) # Creating the legend of the bars in the plot plt.legend(labels = [&#39;GDP_per_capita&#39;]) # Remove y the axis plt.yticks([]) # plt.savefig(&#39;test_bar_plot.png&#39;,dpi=300,bbox_inches=&#39;tight&#39;) # Show plot plt.show() # Saving the plot as a &#39;png&#39; #plt.savefig(&#39;testbarplot.png&#39;) 2.5 Stacked bar charts labels = [&#39;Group 1&#39;, &#39;Group 2&#39;, &#39;Group 3&#39;, &#39;Group 4&#39;, &#39;Group 5&#39;] men_means = [20, 35, 30, 35, 27] women_means = [25, 32, 34, 20, 25] men_std = [2, 3, 4, 1, 2] women_std = [3, 5, 2, 3, 3] width = 0.7 # the width of the bars: can also be len(x) sequence fig, ax = plt.subplots() ax.bar(labels, men_means, width, yerr=men_std, label=&#39;Men&#39;) ax.bar(labels, women_means, width, yerr=women_std, bottom=men_means, label=&#39;Women&#39;) ax.set_ylabel(&#39;Scores&#39;) ax.set_title(&#39;Scores by group and gender&#39;) ax.legend() plt.show() 2.6 Line charts import matplotlib.ticker as mtick # Note: you can also get the same result without using a pandas dataframe #Year = [1920,1930,1940,1950,1960,1970,1980,1990,2000,2010] #Unemployment_Rate = [9.8,12,8,7.2,6.9,7,6.5,6.2,5.5,6.3] #Using a pandas dataframe Data = {&#39;Year&#39;: [1920,1930,1940,1950,1960,1970,1980,1990,2000,2010], &#39;Unemployment_Rate&#39;: [9.8,12,8,7.2,6.9,7,6.5,6.2,5.5,6.3] } df = pd.DataFrame(Data,columns=[&#39;Year&#39;,&#39;Unemployment_Rate&#39;]) #Add in a % sign to a new variable #df[&#39;Unemployment_Rate_Percent&#39;] = df[&#39;Unemployment_Rate&#39;].astype(str) + &#39;%&#39; plt.plot(df[&#39;Year&#39;], df[&#39;Unemployment_Rate&#39;], color=&#39;blue&#39;, marker=&#39;o&#39;) plt.title(&#39;Unemployment rate (1920-2010)&#39;, fontsize=12) plt.xlabel(&#39;Year&#39;, fontsize=12) plt.ylabel(&#39;&#39;, fontsize=12) #plt.grid(False) plt.gca().yaxis.grid(True) plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter()) plt.show() 2.7 Scatter plot x =[5, 7, 8, 7, 2, 17, 2, 9, 4, 11, 12, 9, 6] y =[99, 86, 87, 88, 100, 86, 103, 87, 94, 78, 77, 85, 86] plt.scatter(x, y, c =&quot;blue&quot;) plt.title(&#39;Scatterplot title&#39;, fontsize=12) plt.xlabel(&#39;x label&#39;, fontsize=12) plt.ylabel(&#39;y label&#39;, fontsize=12) plt.show() 2.8 Histogram np.random.seed(99) # Using the format np.random.normal(mu, sigma, 1000) x = np.random.normal(0,1,size=1000) # Use density=False for counts, and density=True for probability plt.hist(x, density=False, bins=100) # Plot mean line plt.axvline(x.mean(), color=&#39;k&#39;, linestyle=&#39;dashed&#39;, linewidth=1) plt.ylabel(&#39;Probability&#39;) plt.xlabel(&#39;Mean&#39;); plt.show() 2.9 Multiple charts in single plot fig, (ax,ax2) = plt.subplots(ncols=2) ax.plot([0,1],[-35,30]) ax.yaxis.set_major_formatter(EngFormatter(unit=u&quot;°C&quot;)) ax2.plot([0,1],[-35,30]) ax2.yaxis.set_major_formatter(StrMethodFormatter(u&quot;{x:.0f} °C&quot;)) plt.tight_layout() plt.show() 2.10 Annotating charts Example taken from the wonderful blog at Practical Economics. plt.title(&#39;Employment Impact of a Minimum Wage&#39;) # Set limits of chart plt.xlim(10,70) plt.ylim(130,200) # Wage supply floor plt.plot([10,30],[150,150],color=&#39;orange&#39;) plt.text(10.5,140.0,&quot;Marginal\\nDisutility\\nof Labour&quot;,size=8,color=&#39;black&#39;) plt.plot([10,40],[160,160],color=&#39;lightgrey&#39;,linestyle=&#39;--&#39;) plt.plot([40,40],[130,160],color=&#39;lightgrey&#39;,linestyle=&#39;--&#39;) plt.annotate(&#39;&#39;, xy=(30,138),xytext=(40,138),arrowprops = dict(arrowstyle=&#39;&lt;-&gt;&#39;)) plt.text(31,140,&quot;Employment\\nLoss&quot;,size=8, color=&#39;k&#39;) plt.axhspan(170,150,xmin=0.0,xmax=20/60,alpha=0.9,color=&#39;dodgerblue&#39;) plt.annotate(&#39;Additional Surplus to\\nEmployed&#39;, xy=(20,162),xytext=(30,185),arrowprops = dict(arrowstyle=&#39;-&gt;&#39;)) # Deadweight loss triangles trianglex=[30,30,40,30] triangley=[150,170,160,150] plt.plot(trianglex,triangley, color=&#39;grey&#39;) plt.fill(trianglex,triangley,color=&#39;grey&#39;) # Main box plt.plot([10,30],[170,170],&#39;tab:orange&#39;) plt.plot([30,30],[130,170],&#39;tab:green&#39;) #plt.plot([50,50],[130,170],&#39;tab:red&#39;) plt.text(11,171,&quot;Wage Rate&quot;,size=8,color=&#39;black&#39;) plt.annotate(&#39;Deadweight\\nLoss&#39;, xy=(32,162),xytext=(38,175),arrowprops = dict(arrowstyle=&#39;-&gt;&#39;)) #Labour Demand Curve plt.plot([20,60],[180,140],color=&#39;tab:grey&#39;) plt.text(61,135,&quot;Marginal\\nProduct\\nof Labour\\nDemand&quot;,size=8,color=&#39;black&#39;) #Labour Supply Curve plt.plot([20,60],[140,180],color=&#39;tab:grey&#39;) plt.text(61,180,&quot;Labour\\nSupply&quot;,size=8,color=&#39;k&#39;) plt.show() "],["making-maps-with-python.html", "Chapter 3 Making maps with python 3.1 Importing python packages 3.2 Making simple maps with geopandas 3.3 Geocoding address data 3.4 Open Street Map", " Chapter 3 Making maps with python Maps are a great way to communicate data. They’re easily understandable, flexible, and more intuitive than a chart. There’s been numerous studies showing that the average professional often struggles to interpret the units on a y-axis, let alone understand trends in scatter or line graphs. Making maps in R takes some initial investment (note: they can be fiddly). However once you have some code you know and understand, spinning up new pieces of analysis can happen in minutes, rather than hours or days. The aim of this quick-reference guide is to get you from ‘I can produce a map in R’ to something more like ‘I can conduct spatial analysis and produce a visual which is ready to send without any further work.’ knitr::opts_chunk$set(echo = TRUE) library(reticulate) # py_install is a special wrapper from the reticulate package that does &quot;conda install&quot; automatically use_condaenv(&quot;tf&quot;) reticulate::py_install(&quot;geopy&quot;) reticulate::py_install(&quot;geocoder&quot;) 3.1 Importing python packages Let’s load in some libraries that we will use again and again when making charts. import matplotlib.pyplot as plt import matplotlib.dates as mdates import pandas as pd import geopandas as gpd import numpy as np import statistics from scipy.stats import norm from matplotlib.ticker import EngFormatter, StrMethodFormatter 3.2 Making simple maps with geopandas Just like a pandas dataframe, the geopandas package allows us to us shapefiles. We’ll go ahead and download some shapefiles from the ABS. # Read the SHP file SA4_shp = gpd.read_file(&#39;ASGS/SA4_2021_AUST_SHP_GDA2020/SA4_2021_AUST_GDA2020.shp&#39;) # Load the data using Geopandas SA4_shp.head() # Check the coordinate reference system attached to the shapefile SA4_shp.crs # Filter the data for only Greater Melbourne SA4_shp_MEL = SA4_shp[SA4_shp[&#39;GCC_NAME21&#39;]==&#39;Greater Melbourne&#39;] SA4_shp_MEL.head() # Quick plot of the shapefile SA4_shp_MEL.plot(figsize=(20, 20), linewidth=0.1, edgecolor=&#39;0.9&#39;, legend = True) plt.annotate(&#39;Melbourne\\nCBD&#39;, xy=(144.96246,-37.81214), xytext=(144.46246,-38.21), arrowprops = dict(arrowstyle=&#39;-&#39;)) plt.title(&quot;SA2&#39;s of Greater Melbourne&quot;, fontsize=12) plt.gca().axis(&#39;off&#39;) plt.show() Here’s another example using a shapefile for WA # Load Geometry File WA_shp = gpd.read_file(&#39;data/NOV21_WA_LOC_POLYGON_shp_GDA2020/wa_localities.shp&#39;) WA_shp.plot(figsize=(20, 20), linewidth=0.1, color=&#39;green&#39;, edgecolor=&#39;0.9&#39;, legend = True) plt.title(&quot;Western Australia&quot;, fontsize=12) plt.gca().axis(&#39;off&#39;) plt.show() 3.3 Geocoding address data Using Nominatim to find the coordinates of a street address from geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=&quot;coverdale&quot;) test_location = geolocator.geocode(&quot;150 Collins Street, Melbourne Australia&quot;) print(test_location.address) print(test_location.latitude, test_location.longitude) print(test_location.raw) Using Nominatim to find the street address from a set of coordinates from geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=&quot;coverdale&quot;) test_location = geolocator.reverse(&quot;-37.81214, 144.96246&quot;) print(test_location.address) print(test_location.latitude, test_location.longitude) print(test_location.raw) We can also use geopy to find the distance between two points Geopy can calculate geodesic distance between two points using the geodesic distance or the great-circle distance, with a default of the geodesic distance available as the function geopy.distance.distance. #Here&#39;s an example usage of the geodesic distance: from geopy.distance import geodesic sydney = (-37.81214, 144.96246) melbourne = (-33.8688, 151.2093) print(geodesic(sydney, melbourne).kilometers) # Using great-circle distance: from geopy.distance import great_circle sydney = (-37.81214, 144.96246) melbourne = (-33.8688, 151.2093) print(great_circle(sydney, melbourne).kilometers) Note we see a slight difference in the km measurement (around 500m) - this is due to the earth not being exactly spherical. Geocoding a list of addresses hospital_data_clean = hospital_data.dropna() # Split out the points into latitude and longitude hospital_data_clean[[‘lat,’ ‘lon,’ ‘altitude’]] = pd.DataFrame(hospital_data[‘point’].to_list(), index=hospital_data.index) # View dataframe hospital_data_clean.head(5) # Import necessary modules import geopy import geocoder import geopandas as gpd from shapely.geometry import Point from geopandas.tools import geocode from geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=&quot;coverdale&quot;) from geopy.extra.rate_limiter import RateLimiter geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1) # Read the data hospital_data = pd.read_csv(&quot;data/QLD_public_hospitals.csv&quot;, on_bad_lines=&#39;skip&#39;, encoding=&#39;unicode_escape&#39;) hospital_data.head(5) # Add the state and country to the data hospital_data[&#39;Address&#39;] = hospital_data[&#39;Address&#39;].astype(str) + &quot;, Queensland, Australia&quot; # Find the location hospital_data[&#39;location&#39;] = hospital_data[&#39;Address&#39;].apply(geocode) # Turn the location into a point hospital_data[&#39;point&#39;] = hospital_data[&#39;location&#39;].apply(lambda loc: tuple(loc.point) if loc else None) hospital_data_clean = hospital_data.dropna() # Split out the points into latitude and longitude hospital_data_clean[[&#39;lat&#39;, &#39;lon&#39;, &#39;altitude&#39;]] = pd.DataFrame(hospital_data_clean[&#39;point&#39;].to_list(), index=hospital_data_clean.index) geometry = [Point(xy) for xy in zip (hospital_data_clean[&#39;lon&#39;], hospital_data_clean[&#39;lat&#39;])] hospital_geodataframe = gpd.GeoDataFrame(hospital_data_clean, crs=&quot;EPSG:4326&quot;, geometry=geometry) #hospital_geodataframe.set_crs(epsg=4326, inplace=True) # View dataframe hospital_geodataframe.head(5) Let’s now plot these points on a map of Queensland. We’ll also need to load in the shape of Queensland as the ‘base map.’ # Read the SHP file STE_shp = gpd.read_file(&#39;ASGS/STE_2021_AUST_SHP_GDA2020/STE_2021_AUST_GDA2020.shp&#39;) # Load the data using Geopandas STE_shp.head() # Check the coordinate reference system attached to the shapefile STE_shp.crs # Filter the data for only Greater Melbourne STE_shp_QLD = STE_shp[STE_shp[&#39;STE_NAME21&#39;]==&#39;Queensland&#39;] STE_shp_QLD.head() Now we plot the two layers together fig, ax = plt.subplots(1, 1, figsize=(12, 12)) # Base layer with all the areas for the background STE_shp_QLD.plot(ax=ax, linewidth=0.1, color=&#39;lightgrey&#39;, edgecolor=&#39;0.9&#39;) # Hospital points hospital_geodataframe.plot(ax=ax, alpha=1, facecolor=&#39;blue&#39;, markersize=5) plt.title(&quot;Hospitals in Queensland&quot;, fontsize=12) ax.set_axis_off() plt.show() 3.4 Open Street Map This exercise loosely follows the wonderful tutorial created by Carlos Cilleruelo. It builds off the OSMnx package that allows us to download spatial data from OpenStreetMap. import osmnx as ox center_point = (-37.81214, 144.96246) G = ox.graph_from_point(center_point, dist=15000, retain_all=True, simplify = True, network_type=&#39;all&#39;) #place = [&quot;Melbourne, Australia&quot;] #G = ox.graph_from_place(place, retain_all=True, simplify = True, network_type=&#39;all&#39;) # Unpack the data u = [] v = [] key = [] data = [] for uu, vv, kkey, ddata in G.edges(keys=True, data=True): u.append(uu) v.append(vv) key.append(kkey) data.append(ddata) # Lists to store colors and widths roadColors = [] roadWidths = [] for item in data: if &quot;length&quot; in item.keys(): if item[&quot;length&quot;] &lt;= 100: linewidth = 0.10 color = &quot;#a6a6a6&quot; elif item[&quot;length&quot;] &gt; 100 and item[&quot;length&quot;] &lt;= 200: linewidth = 0.15 color = &quot;#676767&quot; elif item[&quot;length&quot;] &gt; 200 and item[&quot;length&quot;] &lt;= 400: linewidth = 0.25 color = &quot;#454545&quot; elif item[&quot;length&quot;] &gt; 400 and item[&quot;length&quot;] &lt;= 800: color = &quot;#bdbdbd&quot; linewidth = 0.35 else: color = &quot;#d5d5d5&quot; linewidth = 0.45 if &quot;primary&quot; in item[&quot;highway&quot;]: linewidth = 0.5 color = &quot;#ffff&quot; else: color = &quot;#a6a6a6&quot; linewidth = 0.10 roadColors.append(color) roadWidths.append(linewidth) for item in data: if &quot;footway&quot; in item[&quot;highway&quot;]: color = &quot;#ededed&quot; linewidth = 0.25 else: color = &quot;#a6a6a6&quot; linewidth = 0.5 roadWidths.append(linewidth) #Center of the map latitude = -37.81214 longitude = 144.96246 #Limit borders north = latitude + 0.15 south = latitude - 0.15 east = longitude + 0.15 west = longitude - 0.15 bgcolor = &quot;#061529&quot; fig, ax = ox.plot_graph(G, node_size=0, bbox = (north, south, east, west), dpi = 300,bgcolor = bgcolor, save = False, edge_color=roadColors, edge_linewidth=roadWidths, edge_alpha=1) fig.tight_layout(pad=0) fig.savefig(&quot;madrid.png&quot;, dpi=300, bbox_inches=&#39;tight&#39;, format=&quot;png&quot;, facecolor=fig.get_facecolor(), transparent=False) fig, ax = ox.plot_graph(G, node_size=0,figsize=(27, 40), dpi = 300,bgcolor = bgcolor, save = False, edge_color=roadColors, edge_linewidth=roadWidths, edge_alpha=1) fig.tight_layout(pad=0) fig.savefig(&quot;madridPoster.png&quot;, dpi=300, format=&quot;png&quot;, bbox_inches=&#39;tight&#39;, facecolor=fig.get_facecolor(), transparent=False) We can also add a water layer for the map above (and combine them in photoshop or similiar) import networkx as nx import osmnx as ox center_point = (-37.81214, 144.96246) G1 = ox.graph_from_point(center_point, dist=15000, dist_type=&#39;bbox&#39;, network_type=&#39;all&#39;, simplify=True, retain_all=True, truncate_by_edge=False, clean_periphery=False, custom_filter=&#39;[&quot;natural&quot;~&quot;water&quot;]&#39;) G2 = ox.graph_from_point(center_point, dist=15000, dist_type=&#39;bbox&#39;, network_type=&#39;all&#39;, simplify=True, retain_all=True, truncate_by_edge=False, clean_periphery=False, custom_filter=&#39;[&quot;waterway&quot;~&quot;river&quot;]&#39;) Gwater = nx.compose(G1, G2) u = [] v = [] key = [] data = [] for uu, vv, kkey, ddata in Gwater.edges(keys=True, data=True): u.append(uu) v.append(vv) key.append(kkey) data.append(ddata) # List to store colors roadColors = [] roadWidths = [] # #72b1b1 # #5dc1b9 for item in data: if &quot;name&quot; in item.keys(): if item[&quot;length&quot;] &gt; 400: color = &quot;#72b1b1&quot; linewidth = 2 else: color = &quot;#72b1b1&quot; linewidth = 0.5 else: color = &quot;#72b1b1&quot; linewidth = 0.5 roadColors.append(color) roadWidths.append(linewidth) fig, ax = ox.plot_graph(Gwater, node_size=0,figsize=(27, 40), dpi = 300, save = False, edge_color=roadColors, edge_linewidth=roadWidths, edge_alpha=1) fig.tight_layout(pad=0) fig.savefig(&quot;water.png&quot;, dpi=300, format=&quot;png&quot;, bbox_inches=&#39;tight&#39;, facecolor=fig.get_facecolor(), transparent=True) "],["hypothesis-testing.html", "Chapter 4 Hypothesis testing 4.1 Importing a new module 4.2 Importing python packages 4.3 Correlation 4.4 Stationary tests 4.5 t-test of the sample mean 4.6 Testing for normality", " Chapter 4 Hypothesis testing knitr::opts_chunk$set(echo = TRUE) library(reticulate) # py_install is a special wrapper from the reticulate package that does &quot;conda install&quot; automatically py_install(&quot;statsmodels&quot;) use_condaenv(&quot;tf&quot;) 4.1 Importing a new module ## *Should not be needed* # Use conda install *package_name* to install a new module (in terminal) # Use reticulate::py_install(&quot;package_name&quot;) to load in that specific package to the R Studio environment # Then use import &quot;package_name&quot; to import the package into the relevant chunk 4.2 Importing python packages import matplotlib.pyplot as plt import matplotlib.dates as mdates import pandas as pd import numpy as np import statistics import statsmodels.api as sm from scipy.stats import norm from matplotlib.ticker import EngFormatter, StrMethodFormatter 4.3 Correlation Pearson’s Correlation Coefficient Tests whether two samples have a linear relationship. Assumptions Observations in each sample are independent and identically distributed (iid). Observations in each sample are normally distributed. Observations in each sample have the same variance. Interpretation H0: the two samples are independent. H1: there is a dependency between the samples. # Example of the Pearson&#39;s Correlation test from scipy.stats import pearsonr data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869] data2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579] # Run the test stat, p = pearsonr(data1, data2) print(&#39;stat=%.3f, p=%.3f&#39; % (stat, p)) if p &gt; 0.05: print(&#39;Datasets are not correlated&#39;) else: print(&#39;Datasets are correlated&#39;) 4.4 Stationary tests Augmented Dickey-Fuller Unit Root Test: This tests whether a time series has a unit root, e.g. has a trend or more generally is autoregressive. Assumptions Observations in are temporally ordered. Interpretation H0: a unit root is present (series is non-stationary). H1: a unit root is not present (series is stationary). # Example of the Augmented Dickey-Fuller unit root test from statsmodels.tsa.stattools import adfuller data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # Run the test stat, p, lags, obs, crit, t = adfuller(data) print(&#39;stat=%.3f, p=%.3f&#39; % (stat, p)) if p &gt; 0.05: print(&#39;Series is not stationary&#39;) else: print(&#39;Series is stationary&#39;) 4.5 t-test of the sample mean Tests whether the means of two independent samples are significantly different. Assumptions Observations in each sample are independent and identically distributed (iid). Observations in each sample are normally distributed. Observations in each sample have the same variance. Interpretation H0: the means of the samples are equal. H1: the means of the samples are unequal. # Example of the Student&#39;s t-test from scipy.stats import ttest_ind data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869] data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169] # Run the test stat, p = ttest_ind(data1, data2) print(&#39;stat=%.3f, p=%.3f&#39; % (stat, p)) if p &gt; 0.05: print(&#39;Probably the same distribution&#39;) else: print(&#39;Probably different distributions&#39;) We can verify this result by using a normal distribution. For this, we use the function: np.random.normal(mu, sigma, 1000) # Create a normnal distribution with mean of 0 and a variance of 1 mu_0, sigma_0 = 0, 1 normal_dist_0 = np.random.normal(mu_0, sigma_0, 1000) # Chart count, bins, ignored = plt.hist(normal_dist_0, 30, density=True) plt.plot(bins, 1/(sigma_0 * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu_0)**2 / (2 * sigma_0**2) ), linewidth=2, color=&#39;orange&#39;) plt.show() Now we’ll re-run the code, but change the sample mean to 1 rather than 0. # Create a normnal distribution with mean of 1 and a variance of 1 mu_1, sigma_1 = 1, 1 normal_dist_1 = np.random.normal(mu_1, sigma_1, 1000) # Chart count, bins, ignored = plt.hist(normal_dist_1, 30, density=True) plt.plot(bins, 1/(sigma_1+1 * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu_1)**2 / (2 * sigma_1**2) ), linewidth=2, color=&#39;orange&#39;) plt.show() 4.6 Testing for normality The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution. Interpretation H0: sample is from a normal distribution H1: sample is not from a normal distribution from scipy import stats stats.shapiro(normal_dist_0) stats.shapiro(normal_dist_1) We can see here that the p-values are huge (well above 0.05) - and we fail to reject the null hypothesis. "],["regression-analysis.html", "Chapter 5 Regression analysis 5.1 Importing python packages 5.2 Create a model and fit it 5.3 Polynomial regression", " Chapter 5 Regression analysis knitr::opts_chunk$set(echo = TRUE) library(reticulate) use_condaenv(&quot;tf&quot;) 5.1 Importing python packages import matplotlib.pyplot as plt import matplotlib.dates as mdates import pandas as pd import numpy as np import statistics from scipy.stats import norm from matplotlib.ticker import EngFormatter, StrMethodFormatter The fundamental data type of NumPy is the array type called numpy.ndarray. The rest of this article uses the term array to refer to instances of the type numpy.ndarray. from sklearn.datasets import fetch_california_housing california_housing = fetch_california_housing(as_frame=True) print(california_housing.DESCR) california_housing.data.head() #Looks good - let&#39;s convert it into a pandas dataframe california_housing_df = pd.DataFrame(california_housing.data) print(california_housing_df) california_housing.frame.hist(figsize=(12, 10), bins=30, edgecolor=&quot;black&quot;) plt.subplots_adjust(hspace=0.7, wspace=0.4) plt.show() 5.2 Create a model and fit it The next step is to create the regression model as an instance of LinearRegression and fit it with .fit(). import sklearn from sklearn.linear_model import LinearRegression from sklearn import linear_model # Choose our variables of interest x = california_housing_df[[&#39;HouseAge&#39;]] y = california_housing_df[[&#39;MedInc&#39;]] # Make a model model = LinearRegression().fit(x, y) # Analyse the model fit r_sq = model.score(x, y) print(&#39;coefficient of determination:&#39;, r_sq) print(&#39;intercept:&#39;, model.intercept_) print(&#39;slope:&#39;, model.coef_) 5.3 Polynomial regression We can fit different order polynomials by defining the relevant polynomial functions. # Load in relevant packages from numpy import arange from pandas import read_csv from scipy.optimize import curve_fit from matplotlib import pyplot # Define the true objective function for a linear estimation def objective(x, a, b): return a * x + b # load the dataset url = &#39;https://raw.githubusercontent.com/jbrownlee/Datasets/master/longley.csv&#39; dataframe = read_csv(url, header=None) data = dataframe.values # choose the input and output variables x, y = data[:, 4], data[:, -1] # curve fit popt, _ = curve_fit(objective, x, y) # summarize the parameter values a, b = popt print(&#39;y = %.5f * x + %.5f&#39; % (a, b)) # plot input vs output plt.scatter(x, y, c =&quot;blue&quot;) # define a sequence of inputs between the smallest and largest known inputs x_line = arange(min(x), max(x), 1) # calculate the output for the range y_line = objective(x_line, a, b) # create a line plot for the mapping function plt.plot(x_line, y_line, label=&#39;Polynomial&#39;, color=&#39;purple&#39;, alpha=1, linewidth=1.2, linestyle=&#39;dashed&#39;) plt.title(&#39;Using a linear model to approximate data&#39;, fontsize=12) plt.xlabel(&#39;&#39;, fontsize=10) plt.ylabel(&#39;&#39;, fontsize=10) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(True) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.tick_params( axis=&#39;x&#39;, # changes apply to the x-axis which=&#39;both&#39;, # both major and minor ticks are affected bottom=False, # ticks along the bottom edge are off top=False, # ticks along the top edge are off labelbottom=True) # labels along the bottom edge are off plt.tick_params( axis=&#39;y&#39;, # changes apply to the y-axis which=&#39;both&#39;, # both major and minor ticks are affected left=False, # ticks along the bottom edge are off right=False, # ticks along the top edge are off labelleft=True) # labels along the bottom edge are off plt.grid(False) plt.gca().yaxis.grid(True) plt.legend(fancybox=False, framealpha=1, shadow=False, borderpad=1) plt.savefig(&#39;linear_model_chart.png&#39;,dpi=300,bbox_inches=&#39;tight&#39;) plt.show() Now let’s try a polynomial model # Fit a second degree polynomial to the economic data from numpy import arange from pandas import read_csv from scipy.optimize import curve_fit from matplotlib import pyplot # Define the true objective function def objective(x, a, b, c): return a * x + b * x**2 + c # Load the dataset url = &#39;https://raw.githubusercontent.com/jbrownlee/Datasets/master/longley.csv&#39; dataframe = read_csv(url, header=None) data = dataframe.values # choose the input and output variables x, y = data[:, 4], data[:, -1] # curve fit popt, _ = curve_fit(objective, x, y) # summarize the parameter values a, b, c = popt print(&#39;y = %.5f * x + %.5f * x^2 + %.5f&#39; % (a, b, c)) # plot input vs output plt.scatter(x, y, c =&quot;blue&quot;) # define a sequence of inputs between the smallest and largest known inputs x_line = arange(min(x), max(x), 1) # calculate the output for the range y_line = objective(x_line, a, b, c) # create a line plot for the mapping function # create a line plot for the mapping function plt.plot(x_line, y_line, label=&#39;Polynomial&#39;, color=&#39;purple&#39;, alpha=1, linewidth=1.2, linestyle=&#39;dashed&#39;) plt.title(&#39;Using a polynomial model to approximate data&#39;, fontsize=12) plt.xlabel(&#39;&#39;, fontsize=10) plt.ylabel(&#39;&#39;, fontsize=10) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(True) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.tick_params( axis=&#39;x&#39;, # changes apply to the x-axis which=&#39;both&#39;, # both major and minor ticks are affected bottom=False, # ticks along the bottom edge are off top=False, # ticks along the top edge are off labelbottom=True) # labels along the bottom edge are off plt.tick_params( axis=&#39;y&#39;, # changes apply to the y-axis which=&#39;both&#39;, # both major and minor ticks are affected left=False, # ticks along the bottom edge are off right=False, # ticks along the top edge are off labelleft=True) # labels along the bottom edge are off plt.grid(False) plt.gca().yaxis.grid(True) plt.legend(fancybox=False, framealpha=1, shadow=False, borderpad=1) plt.savefig(&#39;linear_model_chart.png&#39;,dpi=300,bbox_inches=&#39;tight&#39;) plt.show() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
