[["index.html", "Python Cookbook for the Casual Dabbler Chapter 1 Welcome 1.1 Using this book 1.2 Additional resources 1.3 Limitations 1.4 Why write a python book using the R publishing language? 1.5 About the author", " Python Cookbook for the Casual Dabbler Charles Coverdale 2022-04-17 Chapter 1 Welcome G’day and welcome to Python cookbook for the casual dabbler. Some history: I use the R language a lot for work and for side projects. However I’ll be the first to admit - for some things, python is simply much better. Rather than storing my bits and bobs of python code in random files and repos, I’ve created this book to systematize key the key processes I use often, in one convenient location. 1.1 Using this book In each chapter I’ve written up the background, methodology and code for a separate piece of analysis. Most of this code will not be extraordinary to the seasoned Python aficionado. The vast majority can be found elsewhere if you dig around on stackexchange or read some of the many python programming blogs and books. However I find that in classic Pareto style ~20% of my code contributes to the vast majority of my work output. Having this on hand will hopefully be useful to both myself and others. 1.2 Additional resources The Python community is continually writing new books and package documentation with great worked examples. Some of my favourites are: Python Beginners Guide Python Data Science Handbook Coding for Economists 1.3 Limitations I’ll be honest with you - there’s bound to be bugs galore in this. If you find one (along with spelling errors etc) please email me at charlesfcoverdale@gmail.com with the subject line ‘Python Cookbook for the Casual Dabbler.’ 1.4 Why write a python book using the R publishing language? It’s a fair question. The simplest answer is that the R Markdown and Bookdown languages are excellent - and far more conducive to longer form content compared to jupyter notebooks. There are some quirks however. For example, to get python to run within the RStudio IDE I need to load up each chapter with some code (written in R) that looks something like this: library(reticulate) use_condaenv(\"tf\") reticulate::py_install(\"numpy\") reticulate::py_install(\"geopy\") Interested in publishing your own book using R Markdown or Bookdown? There’s some excellent books on the topic, however I find this guide as by far the most useful. 1.5 About the author Charles Coverdale is an economist based in Melbourne, Australia. He is passionate about economics, climate science, and building talented teams. You can get in touch with Charles on twitter to hear more about his current projects. "],["making-beautiful-charts-in-python.html", "Chapter 2 Making beautiful charts in Python 2.1 Importing python packages 2.2 Reading and cleaning data 2.3 Line charts 2.4 Bar charts 2.5 Stacked bar charts 2.6 Line charts (from raw data) 2.7 Scatter plot 2.8 Histogram 2.9 Multiple charts in single plot 2.10 Annotating charts 2.11 Mimicking The Economist", " Chapter 2 Making beautiful charts in Python This chapter contains the code for some of my most used charts and visualization techniques. 2.1 Importing python packages Let’s load in some libraries that we will use again and again when making charts. import matplotlib.pyplot as plt import matplotlib.dates as mdates import pandas as pd import numpy as np import statistics from scipy.stats import norm from matplotlib.ticker import EngFormatter, StrMethodFormatter 2.2 Reading and cleaning data Let’s start by importing data from a csv and making it usable. In this example, we’ll use the weather profile from 2019 in Melbourne, Australia. We’ll also create a new column for a rolling average of the temperature. #Note non-ascii character in csv will stuff up the import, so we add this term: encoding=&#39;unicode_escape&#39; # Note: The full file location is this: # /Users/charlescoverdale/Documents/2021/Python_code_projects/learning_journal_v0-1/MEL_weather_2019.csv # Import csv df_weather= pd.read_csv(&quot;MEL_weather_2019.csv&quot;,encoding=&#39;unicode_escape&#39;) # Create a single data column and bind to df df_weather[&#39;Date&#39;] = pd.to_datetime(df_weather[[&#39;Year&#39;, &#39;Month&#39;, &#39;Day&#39;]]) # Drop the original three field date columns df_weather = df_weather.drop(columns=[&#39;Year&#39;, &#39;Month&#39;, &#39;Day&#39;]) # Let&#39;s change the name of the solar exposure column df_weather = df_weather.rename({&#39;Daily global solar exposure (MJ/m*m)&#39;:&#39;Solar_exposure&#39;, &#39;Rainfall amount (millimetres)&#39;:&#39;Rainfall&#39;, &#39;Maximum temperature (°C)&#39;: &#39;Max_temp&#39;}, axis=1) #Add a rolling average df_weather[&#39;Rolling_avg&#39;] = df_weather[&#39;Max_temp&#39;].rolling(window=7).mean() df_weather.head() 2.3 Line charts Now that the data is in a reasonable format (e.g. there is a simple to use ‘Date’ column), let’s go ahead and make a line chart. # Now let&#39;s plot maximum temperature on a line chart plt.plot(df_weather[&#39;Date&#39;], df_weather[&#39;Max_temp&#39;], label=&#39;Maximum temperature&#39;, color=&#39;blue&#39;, alpha=0.2, linewidth=1.0, marker=&#39;&#39;) plt.plot(df_weather[&#39;Date&#39;], df_weather[&#39;Rolling_avg&#39;], label=&#39;7-day moving average&#39;, color=&#39;red&#39;, linewidth=1.0, marker=&#39;&#39;) plt.title(&#39;Maximum temperature in Melbourne (2019)&#39;, fontsize=12) plt.xlabel(&#39;&#39;, fontsize=10) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b&#39;)) plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) #plt.margins(x=0) plt.ylabel(&#39;&#39;, fontsize=10) plt.gca().yaxis.set_major_formatter(StrMethodFormatter(u&quot;{x:.0f}°C&quot;)) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(True) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.tick_params( axis=&#39;x&#39;, # changes apply to the x-axis which=&#39;both&#39;, # both major and minor ticks are affected bottom=False, # ticks along the bottom edge are off top=False, # ticks along the top edge are off labelbottom=True) # labels along the bottom edge are off plt.tick_params( axis=&#39;y&#39;, # changes apply to the y-axis which=&#39;both&#39;, # both major and minor ticks are affected left=False, # ticks along the bottom edge are off right=False, # ticks along the top edge are off labelleft=True) # labels along the bottom edge are off plt.grid(False) plt.gca().yaxis.grid(True) plt.legend(fancybox=False, framealpha=1, shadow=False, borderpad=1) plt.savefig(&#39;weather_chart_save.png&#39;,dpi=300,bbox_inches=&#39;tight&#39;) plt.show() 2.4 Bar charts # Chart 1: Bar plot # Get data country = [&#39;USA&#39;, &#39;Canada&#39;, &#39;Germany&#39;, &#39;UK&#39;, &#39;France&#39;] GDP_per_capita = [45,40,38,16,10] # Create plot plt.bar(country, GDP_per_capita, width=0.8, align=&#39;center&#39;,color=&#39;blue&#39;, edgecolor = &#39;black&#39;) # Labels and titles plt.title(&#39;GDP per capita of select OECD countries&#39;) plt.xlabel(&#39;Test x label&#39;) plt.ylabel(&#39;&#39;) #A dd bar annotations to barchart # Location for the annotated text i = 1.0 j = 1.0 # Annotating the bar plot with the values (total death count) for i in range(len(country)): plt.annotate(GDP_per_capita[i], (-0.1 + i, GDP_per_capita[i] + j)) # Creating the legend of the bars in the plot plt.legend(labels = [&#39;GDP_per_capita&#39;]) # Remove y the axis plt.yticks([]) # plt.savefig(&#39;test_bar_plot.png&#39;,dpi=300,bbox_inches=&#39;tight&#39;) # Show plot plt.show() # Saving the plot as a &#39;png&#39; #plt.savefig(&#39;testbarplot.png&#39;) 2.5 Stacked bar charts labels = [&#39;Group 1&#39;, &#39;Group 2&#39;, &#39;Group 3&#39;, &#39;Group 4&#39;, &#39;Group 5&#39;] men_means = [20, 35, 30, 35, 27] women_means = [25, 32, 34, 20, 25] men_std = [2, 3, 4, 1, 2] women_std = [3, 5, 2, 3, 3] width = 0.7 # the width of the bars: can also be len(x) sequence fig, ax = plt.subplots() ax.bar(labels, men_means, width, yerr=men_std, label=&#39;Men&#39;) ax.bar(labels, women_means, width, yerr=women_std, bottom=men_means, label=&#39;Women&#39;) ax.set_ylabel(&#39;Scores&#39;) ax.set_title(&#39;Scores by group and gender&#39;) ax.legend() plt.show() 2.6 Line charts (from raw data) import matplotlib.ticker as mtick # Note: you can also get the same result without using a pandas dataframe #Year = [1920,1930,1940,1950,1960,1970,1980,1990,2000,2010] #Unemployment_Rate = [9.8,12,8,7.2,6.9,7,6.5,6.2,5.5,6.3] #Using a pandas dataframe Data = {&#39;Year&#39;: [1920,1930,1940,1950,1960,1970,1980,1990,2000,2010], &#39;Unemployment_Rate&#39;: [9.8,12,8,7.2,6.9,7,6.5,6.2,5.5,6.3] } df = pd.DataFrame(Data,columns=[&#39;Year&#39;,&#39;Unemployment_Rate&#39;]) #Add in a % sign to a new variable #df[&#39;Unemployment_Rate_Percent&#39;] = df[&#39;Unemployment_Rate&#39;].astype(str) + &#39;%&#39; plt.plot(df[&#39;Year&#39;], df[&#39;Unemployment_Rate&#39;], color=&#39;blue&#39;, marker=&#39;o&#39;) plt.title(&#39;Unemployment rate (1920-2010)&#39;, fontsize=12) plt.xlabel(&#39;Year&#39;, fontsize=12) plt.ylabel(&#39;&#39;, fontsize=12) #plt.grid(False) plt.gca().yaxis.grid(True) plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter()) plt.show() 2.7 Scatter plot x =[5, 7, 8, 7, 2, 17, 2, 9, 4, 11, 12, 9, 6] y =[99, 86, 87, 88, 100, 86, 103, 87, 94, 78, 77, 85, 86] plt.scatter(x, y, c =&quot;blue&quot;) plt.title(&#39;Scatterplot title&#39;, fontsize=12) plt.xlabel(&#39;x label&#39;, fontsize=12) plt.ylabel(&#39;y label&#39;, fontsize=12) plt.show() 2.8 Histogram np.random.seed(99) # Using the format np.random.normal(mu, sigma, 1000) x = np.random.normal(0,1,size=1000) # Use density=False for counts, and density=True for probability plt.hist(x, density=False, bins=100) # Plot mean line plt.axvline(x.mean(), color=&#39;k&#39;, linestyle=&#39;dashed&#39;, linewidth=1) plt.ylabel(&#39;Probability&#39;) plt.xlabel(&#39;Mean&#39;); plt.show() 2.9 Multiple charts in single plot fig, (ax,ax2) = plt.subplots(ncols=2) ax.plot([0,1],[-35,30]) ax.yaxis.set_major_formatter(EngFormatter(unit=u&quot;°C&quot;)) ax2.plot([0,1],[-35,30]) ax2.yaxis.set_major_formatter(StrMethodFormatter(u&quot;{x:.0f} °C&quot;)) plt.tight_layout() plt.show() 2.10 Annotating charts Example taken from the wonderful blog at Practical Economics. plt.title(&#39;Employment Impact of a Minimum Wage&#39;) # Set limits of chart plt.xlim(10,70) plt.ylim(130,200) # Wage supply floor plt.plot([10,30],[150,150],color=&#39;orange&#39;) plt.text(10.5,140.0,&quot;Marginal\\nDisutility\\nof Labour&quot;,size=8,color=&#39;black&#39;) plt.plot([10,40],[160,160],color=&#39;lightgrey&#39;,linestyle=&#39;--&#39;) plt.plot([40,40],[130,160],color=&#39;lightgrey&#39;,linestyle=&#39;--&#39;) plt.annotate(&#39;&#39;, xy=(30,138),xytext=(40,138),arrowprops = dict(arrowstyle=&#39;&lt;-&gt;&#39;)) plt.text(31,140,&quot;Employment\\nLoss&quot;,size=8, color=&#39;k&#39;) plt.axhspan(170,150,xmin=0.0,xmax=20/60,alpha=0.9,color=&#39;dodgerblue&#39;) plt.annotate(&#39;Additional Surplus to\\nEmployed&#39;, xy=(20,162),xytext=(30,185),arrowprops = dict(arrowstyle=&#39;-&gt;&#39;)) # Deadweight loss triangles trianglex=[30,30,40,30] triangley=[150,170,160,150] plt.plot(trianglex,triangley, color=&#39;grey&#39;) plt.fill(trianglex,triangley,color=&#39;grey&#39;) # Main box plt.plot([10,30],[170,170],&#39;tab:orange&#39;) plt.plot([30,30],[130,170],&#39;tab:green&#39;) #plt.plot([50,50],[130,170],&#39;tab:red&#39;) plt.text(11,171,&quot;Wage Rate&quot;,size=8,color=&#39;black&#39;) plt.annotate(&#39;Deadweight\\nLoss&#39;, xy=(32,162),xytext=(38,175),arrowprops = dict(arrowstyle=&#39;-&gt;&#39;)) #Labour Demand Curve plt.plot([20,60],[180,140],color=&#39;tab:grey&#39;) plt.text(61,135,&quot;Marginal\\nProduct\\nof Labour\\nDemand&quot;,size=8,color=&#39;black&#39;) #Labour Supply Curve plt.plot([20,60],[140,180],color=&#39;tab:grey&#39;) plt.text(61,180,&quot;Labour\\nSupply&quot;,size=8,color=&#39;k&#39;) plt.show() 2.11 Mimicking The Economist The visual storytelling team at The Economist is absolutely world class. Their team is quite public about how they use both R and Python in their data science. Robert Ritz has done an outstanding job at documenting how you can use their style when making charts. The dataset we’ll use is the GDP records from 1960-2020. import pandas as pd import numpy as np import matplotlib.pyplot as plt # This makes out plots higher resolution, which makes them easier to see while building plt.rcParams[&#39;figure.dpi&#39;] = 100 gdp = pd.read_csv(&#39;data/gdp_1960_2020.csv&#39;) gdp.head() # The GDP numbers here are very long. To make them easier to read we can divide the GDP number by 1 trillion. gdp[&#39;gdp_trillions&#39;] = gdp[&#39;gdp&#39;] / 1_000_000_000_000 # Now we can filter for only 2020 and grab the bottom 9. We do this instead of sorting by descending because Matplotlib plots from the bottom to top, so we actually want our data in reverse order. gdp[gdp[&#39;year&#39;] == 2020].sort_values(by=&#39;gdp_trillions&#39;)[-9:] # Setup plot size. #fig, ax = plt.subplots(figsize=(3,6)) plt.rcParams[&quot;figure.figsize&quot;] = (3,6) # Create grid # Zorder tells it which layer to put it on. We are setting this to 1 and our data to 2 so the grid is behind the data. ax.grid(which=&quot;major&quot;, axis=&#39;x&#39;, color=&#39;#758D99&#39;, alpha=0.6, zorder=1) # Remove splines. Can be done one at a time or can slice with a list. ax.spines[[&#39;top&#39;,&#39;right&#39;,&#39;bottom&#39;]].set_visible(False) # Make left spine slightly thicker ax.spines[&#39;left&#39;].set_linewidth(1.1) ax.spines[&#39;left&#39;].set_linewidth(1.1) # Setup data gdp[&#39;country&#39;] = gdp[&#39;country&#39;].replace(&#39;the United States&#39;, &#39;United States&#39;) gdp_bar = gdp[gdp[&#39;year&#39;] == 2020].sort_values(by=&#39;gdp_trillions&#39;)[-9:] # Plot data ax.barh(gdp_bar[&#39;country&#39;], gdp_bar[&#39;gdp_trillions&#39;], color=&#39;#006BA2&#39;, zorder=2) # Set custom labels for x-axis ax.set_xticks([0, 5, 10, 15, 20]) ax.set_xticklabels([0, 5, 10, 15, 20]) # Reformat x-axis tick labels ax.xaxis.set_tick_params(labeltop=True, # Put x-axis labels on top labelbottom=False, # Set no x-axis labels on bottom bottom=False, # Set no ticks on bottom labelsize=11, # Set tick label size pad=-1) # Lower tick labels a bit # Reformat y-axis tick labels ax.set_yticklabels(gdp_bar[&#39;country&#39;], # Set labels again ha = &#39;left&#39;) # Set horizontal alignment to left ax.yaxis.set_tick_params(pad=100, # Pad tick labels so they don&#39;t go over y-axis labelsize=11, # Set label size bottom=False) # Set no ticks on bottom/left # Shrink y-lim to make plot a bit tighter ax.set_ylim(-0.5, 8.5) # Add in line and tag ax.plot([-.35, .87], # Set width of line [1.02, 1.02], # Set height of line transform=fig.transFigure, # Set location relative to plot clip_on=False, color=&#39;#E3120B&#39;, linewidth=.6) ax.add_patch(plt.Rectangle((-.35,1.02), # Set location of rectangle by lower left corner 0.12, # Width of rectangle -0.02, # Height of rectangle. Negative so it goes down. facecolor=&#39;#E3120B&#39;, transform=fig.transFigure, clip_on=False, linewidth = 0)) # Add in title and subtitle ax.text(x=-.35, y=.96, s=&quot;The big leagues&quot;, transform=fig.transFigure, ha=&#39;left&#39;, fontsize=13, weight=&#39;bold&#39;, alpha=.8) ax.text(x=-.35, y=.925, s=&quot;2020 GDP, trillions of USD&quot;, transform=fig.transFigure, ha=&#39;left&#39;, fontsize=11, alpha=.8) # Set source text ax.text(x=-.35, y=.08, s=&quot;&quot;&quot;Source: &quot;GDP of all countries (1960-2020)&quot;&quot;&quot;, transform=fig.transFigure, ha=&#39;left&#39;, fontsize=9, alpha=.7) plt.show() # Export plot as high resolution PNG plt.savefig(&#39;docs/economist_bar.png&#39;, # Set path and filename dpi = 300, # Set dots per inch bbox_inches=&quot;tight&quot;, # Remove extra whitespace around plot facecolor=&#39;white&#39;) # Set background color to white We can do a similar process for line charts. countries = gdp[gdp[&#39;year&#39;] == 2020].sort_values(by=&#39;gdp_trillions&#39;)[-9:][&#39;country&#39;].values countries gdp[&#39;date&#39;] = pd.to_datetime(gdp[&#39;year&#39;], format=&#39;%Y&#39;) # Setup plot size. fig, ax = plt.subplots(figsize=(8,4)) # Create grid # Zorder tells it which layer to put it on. We are setting this to 1 and our data to 2 so the grid is behind the data. ax.grid(which=&quot;major&quot;, axis=&#39;y&#39;, color=&#39;#758D99&#39;, alpha=0.6, zorder=1) # Plot data # Loop through country names and plot each one. for country in countries: ax.plot(gdp[gdp[&#39;country&#39;] == country][&#39;date&#39;], gdp[gdp[&#39;country&#39;] == country][&#39;gdp_trillions&#39;], color=&#39;#758D99&#39;, alpha=0.8, linewidth=3) # Plot US and China separately ax.plot(gdp[gdp[&#39;country&#39;] == &#39;United States&#39;][&#39;date&#39;], gdp[gdp[&#39;country&#39;] == &#39;United States&#39;][&#39;gdp_trillions&#39;], color=&#39;#006BA2&#39;, linewidth=3) ax.plot(gdp[gdp[&#39;country&#39;] == &#39;China&#39;][&#39;date&#39;], gdp[gdp[&#39;country&#39;] == &#39;China&#39;][&#39;gdp_trillions&#39;], color=&#39;#3EBCD2&#39;, linewidth=3) # Remove splines. Can be done one at a time or can slice with a list. ax.spines[[&#39;top&#39;,&#39;right&#39;,&#39;left&#39;]].set_visible(False) # Shrink y-lim to make plot a bit tigheter ax.set_ylim(0, 23) # Set xlim to fit data without going over plot area ax.set_xlim(pd.datetime(1958, 1, 1), pd.datetime(2023, 1, 1)) # Reformat x-axis tick labels ax.xaxis.set_tick_params(labelsize=11) # Set tick label size # Reformat y-axis tick labels ax.set_yticklabels(np.arange(0,25,5), # Set labels again ha = &#39;right&#39;, # Set horizontal alignment to right verticalalignment=&#39;bottom&#39;) # Set vertical alignment to make labels on top of gridline ax.yaxis.set_tick_params(pad=-2, # Pad tick labels so they don&#39;t go over y-axis labeltop=True, # Put x-axis labels on top labelbottom=False, # Set no x-axis labels on bottom bottom=False, # Set no ticks on bottom labelsize=11) # Set tick label size # Add labels for USA and China ax.text(x=.63, y=.67, s=&#39;United States&#39;, transform=fig.transFigure, size=10, alpha=.9) ax.text(x=.7, y=.4, s=&#39;China&#39;, transform=fig.transFigure, size=10, alpha=.9) # Add in line and tag ax.plot([0.12, .9], # Set width of line [.98, .98], # Set height of line transform=fig.transFigure, # Set location relative to plot clip_on=False, color=&#39;#E3120B&#39;, linewidth=.6) ax.add_patch(plt.Rectangle((0.12,.98), # Set location of rectangle by lower left corder 0.04, # Width of rectangle -0.02, # Height of rectangle. Negative so it goes down. facecolor=&#39;#E3120B&#39;, transform=fig.transFigure, clip_on=False, linewidth = 0)) # Add in title and subtitle ax.text(x=0.12, y=.91, s=&quot;Ahead of the pack&quot;, transform=fig.transFigure, ha=&#39;left&#39;, fontsize=13, weight=&#39;bold&#39;, alpha=.8) ax.text(x=0.12, y=.86, s=&quot;Top 9 GDP&#39;s by country, in trillions of USD, 1960-2020&quot;, transform=fig.transFigure, ha=&#39;left&#39;, fontsize=11, alpha=.8) # Set source text ax.text(x=0.12, y=0.01, s=&quot;&quot;&quot;Source: GDP of all countries (1960-2020)&quot;&quot;&quot;, transform=fig.transFigure, ha=&#39;left&#39;, fontsize=9, alpha=.7) # Export plot as high resolution PNG plt.savefig(&#39;docs/economist_line.png&#39;, # Set path and filename dpi = 300, # Set dots per inch bbox_inches=&quot;tight&quot;, # Remove extra whitespace around plot facecolor=&#39;white&#39;) # Set background color to white plt.show() "],["making-maps-with-python.html", "Chapter 3 Making maps with python 3.1 Importing python packages 3.2 Making simple maps with geopandas 3.3 Geocoding address data", " Chapter 3 Making maps with python Maps are a great way to communicate data. They’re easily understandable, flexible, and more intuitive than a chart. There’s been numerous studies showing that the average professional often struggles to interpret the units on a y-axis, let alone understand trends in scatter or line graphs. Making maps in R takes some initial investment (note: they can be fiddly). However once you have some code you know and understand, spinning up new pieces of analysis can happen in minutes, rather than hours or days. The aim of this quick-reference guide is to get you from ‘I can produce a map in R’ to something more like ‘I can conduct spatial analysis and produce a visual which is ready to send without any further work.’ knitr::opts_chunk$set(echo = TRUE) library(reticulate) # py_install is a special wrapper from the reticulate package that does &quot;conda install&quot; automatically use_condaenv(&quot;tf&quot;) reticulate::py_install(&quot;geopy&quot;) reticulate::py_install(&quot;geocoder&quot;) 3.1 Importing python packages Let’s load in some libraries that we will use again and again when making charts. import matplotlib.pyplot as plt import matplotlib.dates as mdates import pandas as pd import geopandas as gpd import numpy as np import statistics from scipy.stats import norm from matplotlib.ticker import EngFormatter, StrMethodFormatter 3.2 Making simple maps with geopandas Just like a pandas dataframe, the geopandas package allows us to us shapefiles. We’ll go ahead and download some shapefiles from the ABS. # Read the SHP file SA4_shp = gpd.read_file(&#39;ASGS/SA4_2021_AUST_SHP_GDA2020/SA4_2021_AUST_GDA2020.shp&#39;) # Load the data using Geopandas SA4_shp.head() # Check the coordinate reference system attached to the shapefile SA4_shp.crs # Filter the data for only Greater Melbourne SA4_shp_MEL = SA4_shp[SA4_shp[&#39;GCC_NAME21&#39;]==&#39;Greater Melbourne&#39;] SA4_shp_MEL.head() # Quick plot of the shapefile SA4_shp_MEL.plot(figsize=(20, 20), linewidth=0.1, edgecolor=&#39;0.9&#39;, legend = True) plt.annotate(&#39;Melbourne\\nCBD&#39;, xy=(144.96246,-37.81214), xytext=(144.46246,-38.21), arrowprops = dict(arrowstyle=&#39;-&#39;)) plt.title(&quot;SA2&#39;s of Greater Melbourne&quot;, fontsize=18) plt.gca().axis(&#39;off&#39;) plt.show() Here’s another example using a shapefile for WA # Load Geometry File WA_shp = gpd.read_file(&#39;data/NOV21_WA_LOC_POLYGON_shp_GDA2020/wa_localities.shp&#39;) WA_shp.plot(figsize=(20, 20), linewidth=0.1, color=&#39;green&#39;, edgecolor=&#39;0.9&#39;, legend = True) plt.title(&quot;Western Australia&quot;, fontsize=18) plt.gca().axis(&#39;off&#39;) plt.show() 3.3 Geocoding address data Using Nominatim to find the coordinates of a street address from geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=&quot;coverdale&quot;) test_location = geolocator.geocode(&quot;150 Collins Street, Melbourne Australia&quot;) print(test_location.address) print(test_location.latitude, test_location.longitude) print(test_location.raw) Using Nominatim to find the street address from a set of coordinates from geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=&quot;coverdale&quot;) test_location = geolocator.reverse(&quot;-37.81214, 144.96246&quot;) print(test_location.address) print(test_location.latitude, test_location.longitude) print(test_location.raw) We can also use geopy to find the distance between two points Geopy can calculate geodesic distance between two points using the geodesic distance or the great-circle distance, with a default of the geodesic distance available as the function geopy.distance.distance. #Here&#39;s an example usage of the geodesic distance: from geopy.distance import geodesic sydney = (-37.81214, 144.96246) melbourne = (-33.8688, 151.2093) print(geodesic(sydney, melbourne).kilometers) # Using great-circle distance: from geopy.distance import great_circle sydney = (-37.81214, 144.96246) melbourne = (-33.8688, 151.2093) print(great_circle(sydney, melbourne).kilometers) Note we see a slight difference in the km measurement (around 500m) - this is due to the earth not being exactly spherical. Geocoding a list of addresses hospital_data_clean = hospital_data.dropna() # Split out the points into latitude and longitude hospital_data_clean[[‘lat,’ ‘lon,’ ‘altitude’]] = pd.DataFrame(hospital_data[‘point’].to_list(), index=hospital_data.index) # View dataframe hospital_data_clean.head(5) # Import necessary modules import geopy import geocoder import geopandas as gpd from shapely.geometry import Point from geopandas.tools import geocode from geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=&quot;coverdale&quot;) from geopy.extra.rate_limiter import RateLimiter geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1) # Read the data hospital_data = pd.read_csv(&quot;data/QLD_public_hospitals.csv&quot;, on_bad_lines=&#39;skip&#39;, encoding=&#39;unicode_escape&#39;) hospital_data.head(5) # Add the state and country to the data hospital_data[&#39;Address&#39;] = hospital_data[&#39;Address&#39;].astype(str) + &quot;, Queensland, Australia&quot; # Find the location hospital_data[&#39;location&#39;] = hospital_data[&#39;Address&#39;].apply(geocode) # Turn the location into a point hospital_data[&#39;point&#39;] = hospital_data[&#39;location&#39;].apply(lambda loc: tuple(loc.point) if loc else None) hospital_data_clean = hospital_data.dropna() # Split out the points into latitude and longitude hospital_data_clean[[&#39;lat&#39;, &#39;lon&#39;, &#39;altitude&#39;]] = pd.DataFrame(hospital_data_clean[&#39;point&#39;].to_list(), index=hospital_data_clean.index) geometry = [Point(xy) for xy in zip (hospital_data_clean[&#39;lon&#39;], hospital_data_clean[&#39;lat&#39;])] hospital_geodataframe = gpd.GeoDataFrame(hospital_data_clean, crs=&quot;EPSG:4326&quot;, geometry=geometry) #hospital_geodataframe.set_crs(epsg=4326, inplace=True) # View dataframe hospital_geodataframe.head(5) Let’s now plot these points on a map of Queensland. We’ll also need to load in the shape of Queensland as the ‘base map.’ # Read the SHP file STE_shp = gpd.read_file(&#39;ASGS/STE_2021_AUST_SHP_GDA2020/STE_2021_AUST_GDA2020.shp&#39;) # Load the data using Geopandas STE_shp.head() # Check the coordinate reference system attached to the shapefile STE_shp.crs # Filter the data for only Greater Melbourne STE_shp_QLD = STE_shp[STE_shp[&#39;STE_NAME21&#39;]==&#39;Queensland&#39;] STE_shp_QLD.head() Now we plot the two layers together fig, ax = plt.subplots(1, 1, figsize=(12, 12)) # Base layer with all the areas for the background STE_shp_QLD.plot(ax=ax, linewidth=0.1, color=&#39;lightgrey&#39;, edgecolor=&#39;0.9&#39;) # Hospital points hospital_geodataframe.plot(ax=ax, alpha=1, facecolor=&#39;blue&#39;, markersize=5) plt.title(&quot;Hospitals in Queensland&quot;, fontsize=12) ax.set_axis_off() plt.show() "],["hypothesis-testing.html", "Chapter 4 Hypothesis testing 4.1 Importing a new module 4.2 Importing python packages 4.3 Correlation 4.4 Stationary tests 4.5 t-test of the sample mean 4.6 Testing for normality", " Chapter 4 Hypothesis testing knitr::opts_chunk$set(echo = TRUE) library(reticulate) # py_install is a special wrapper from the reticulate package that does &quot;conda install&quot; automatically py_install(&quot;statsmodels&quot;) use_condaenv(&quot;tf&quot;) 4.1 Importing a new module ## *Should not be needed* # Use conda install *package_name* to install a new module (in terminal) # Use reticulate::py_install(&quot;package_name&quot;) to load in that specific package to the R Studio environment # Then use import &quot;package_name&quot; to import the package into the relevant chunk 4.2 Importing python packages import matplotlib.pyplot as plt import matplotlib.dates as mdates import pandas as pd import numpy as np import statistics import statsmodels.api as sm from scipy.stats import norm from matplotlib.ticker import EngFormatter, StrMethodFormatter 4.3 Correlation Pearson’s Correlation Coefficient Tests whether two samples have a linear relationship. Assumptions Observations in each sample are independent and identically distributed (iid). Observations in each sample are normally distributed. Observations in each sample have the same variance. Interpretation H0: the two samples are independent. H1: there is a dependency between the samples. # Example of the Pearson&#39;s Correlation test from scipy.stats import pearsonr data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869] data2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579] # Run the test stat, p = pearsonr(data1, data2) print(&#39;stat=%.3f, p=%.3f&#39; % (stat, p)) ## stat=0.688, p=0.028 if p &gt; 0.05: print(&#39;Datasets are not correlated&#39;) else: print(&#39;Datasets are correlated&#39;) ## Datasets are correlated 4.4 Stationary tests Augmented Dickey-Fuller Unit Root Test: This tests whether a time series has a unit root, e.g. has a trend or more generally is autoregressive. Assumptions Observations in are temporally ordered. Interpretation H0: a unit root is present (series is non-stationary). H1: a unit root is not present (series is stationary). # Example of the Augmented Dickey-Fuller unit root test from statsmodels.tsa.stattools import adfuller data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # Run the test stat, p, lags, obs, crit, t = adfuller(data) print(&#39;stat=%.3f, p=%.3f&#39; % (stat, p)) ## stat=2.430, p=0.999 if p &gt; 0.05: print(&#39;Series is not stationary&#39;) else: print(&#39;Series is stationary&#39;) ## Series is not stationary 4.5 t-test of the sample mean Tests whether the means of two independent samples are significantly different. Assumptions Observations in each sample are independent and identically distributed (iid). Observations in each sample are normally distributed. Observations in each sample have the same variance. Interpretation H0: the means of the samples are equal. H1: the means of the samples are unequal. # Example of the Student&#39;s t-test from scipy.stats import ttest_ind data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869] data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169] # Run the test stat, p = ttest_ind(data1, data2) print(&#39;stat=%.3f, p=%.3f&#39; % (stat, p)) ## stat=-0.326, p=0.748 if p &gt; 0.05: print(&#39;Probably the same distribution&#39;) else: print(&#39;Probably different distributions&#39;) ## Probably the same distribution We can verify this result by using a normal distribution. For this, we use the function: np.random.normal(mu, sigma, 1000) # Create a normnal distribution with mean of 0 and a variance of 1 mu_0, sigma_0 = 0, 1 normal_dist_0 = np.random.normal(mu_0, sigma_0, 1000) # Chart count, bins, ignored = plt.hist(normal_dist_0, 30, density=True) plt.plot(bins, 1/(sigma_0 * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu_0)**2 / (2 * sigma_0**2) ), linewidth=2, color=&#39;orange&#39;) plt.show() Now we’ll re-run the code, but change the sample mean to 1 rather than 0. # Create a normnal distribution with mean of 1 and a variance of 1 mu_1, sigma_1 = 1, 1 normal_dist_1 = np.random.normal(mu_1, sigma_1, 1000) # Chart count, bins, ignored = plt.hist(normal_dist_1, 30, density=True) plt.plot(bins, 1/(sigma_1+1 * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu_1)**2 / (2 * sigma_1**2) ), linewidth=2, color=&#39;orange&#39;) plt.show() 4.6 Testing for normality The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution. Interpretation H0: sample is from a normal distribution H1: sample is not from a normal distribution from scipy import stats stats.shapiro(normal_dist_0) stats.shapiro(normal_dist_1) We can see here that the p-values are huge (well above 0.05) - and we fail to reject the null hypothesis. "],["regression-analysis.html", "Chapter 5 Regression analysis 5.1 Importing python packages 5.2 Create a model and fit it 5.3 Polynomial regression 5.4 —", " Chapter 5 Regression analysis knitr::opts_chunk$set(echo = TRUE) library(reticulate) use_condaenv(&quot;tf&quot;) 5.1 Importing python packages import matplotlib.pyplot as plt import matplotlib.dates as mdates import pandas as pd import numpy as np import statistics from scipy.stats import norm from matplotlib.ticker import EngFormatter, StrMethodFormatter The fundamental data type of NumPy is the array type called numpy.ndarray. The rest of this article uses the term array to refer to instances of the type numpy.ndarray. from sklearn.datasets import fetch_california_housing california_housing = fetch_california_housing(as_frame=True) print(california_housing.DESCR) california_housing.data.head() #Looks good - let&#39;s convert it into a pandas dataframe california_housing_df = pd.DataFrame(california_housing.data) print(california_housing_df) california_housing.frame.hist(figsize=(12, 10), bins=30, edgecolor=&quot;black&quot;) plt.subplots_adjust(hspace=0.7, wspace=0.4) plt.show() 5.2 Create a model and fit it The next step is to create the regression model as an instance of LinearRegression and fit it with .fit(). import sklearn from sklearn.linear_model import LinearRegression from sklearn import linear_model # Choose our variables of interest x = california_housing_df[[&#39;HouseAge&#39;]] y = california_housing_df[[&#39;MedInc&#39;]] # Make a model model = LinearRegression().fit(x, y) # Analyse the model fit r_sq = model.score(x, y) print(&#39;coefficient of determination:&#39;, r_sq) ## coefficient of determination: 0.014169090760525749 print(&#39;intercept:&#39;, model.intercept_) ## intercept: [4.38527909] print(&#39;slope:&#39;, model.coef_) ## slope: [[-0.01796848]] 5.3 Polynomial regression We can fit different order polynomials by defining the relevant polynomial functions. # Load in relevant packages from numpy import arange from pandas import read_csv from scipy.optimize import curve_fit from matplotlib import pyplot # Define the true objective function for a linear estimation def objective(x, a, b): return a * x + b # load the dataset url = &#39;https://raw.githubusercontent.com/jbrownlee/Datasets/master/longley.csv&#39; dataframe = read_csv(url, header=None) data = dataframe.values # choose the input and output variables x, y = data[:, 4], data[:, -1] # curve fit popt, _ = curve_fit(objective, x, y) # summarize the parameter values a, b = popt print(&#39;y = %.5f * x + %.5f&#39; % (a, b)) # plot input vs output ## y = 0.48488 * x + 8.38067 plt.scatter(x, y, c =&quot;blue&quot;) # define a sequence of inputs between the smallest and largest known inputs ## &lt;matplotlib.collections.PathCollection object at 0x186f613c0&gt; x_line = arange(min(x), max(x), 1) # calculate the output for the range y_line = objective(x_line, a, b) # create a line plot for the mapping function plt.plot(x_line, y_line, label=&#39;Polynomial&#39;, color=&#39;purple&#39;, alpha=1, linewidth=1.2, linestyle=&#39;dashed&#39;) ## [&lt;matplotlib.lines.Line2D object at 0x186f62050&gt;] plt.title(&#39;Using a linear model to approximate data&#39;, fontsize=12) ## Text(0.5, 1.0, &#39;Using a linear model to approximate data&#39;) plt.xlabel(&#39;&#39;, fontsize=10) ## Text(0.5, 0, &#39;&#39;) plt.ylabel(&#39;&#39;, fontsize=10) ## Text(0, 0.5, &#39;&#39;) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(True) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.tick_params( axis=&#39;x&#39;, # changes apply to the x-axis which=&#39;both&#39;, # both major and minor ticks are affected bottom=False, # ticks along the bottom edge are off top=False, # ticks along the top edge are off labelbottom=True) # labels along the bottom edge are off plt.tick_params( axis=&#39;y&#39;, # changes apply to the y-axis which=&#39;both&#39;, # both major and minor ticks are affected left=False, # ticks along the bottom edge are off right=False, # ticks along the top edge are off labelleft=True) # labels along the bottom edge are off plt.grid(False) plt.gca().yaxis.grid(True) plt.legend(fancybox=False, framealpha=1, shadow=False, borderpad=1) ## &lt;matplotlib.legend.Legend object at 0x187f5d870&gt; plt.savefig(&#39;linear_model_chart.png&#39;,dpi=300,bbox_inches=&#39;tight&#39;) plt.show() Now let’s try a polynomial model # Fit a second degree polynomial to the economic data from numpy import arange from pandas import read_csv from scipy.optimize import curve_fit from matplotlib import pyplot # Define the true objective function def objective(x, a, b, c): return a * x + b * x**2 + c # Load the dataset url = &#39;https://raw.githubusercontent.com/jbrownlee/Datasets/master/longley.csv&#39; dataframe = read_csv(url, header=None) data = dataframe.values # choose the input and output variables x, y = data[:, 4], data[:, -1] # curve fit popt, _ = curve_fit(objective, x, y) # summarize the parameter values a, b, c = popt print(&#39;y = %.5f * x + %.5f * x^2 + %.5f&#39; % (a, b, c)) # plot input vs output plt.scatter(x, y, c =&quot;blue&quot;) # define a sequence of inputs between the smallest and largest known inputs x_line = arange(min(x), max(x), 1) # calculate the output for the range y_line = objective(x_line, a, b, c) # create a line plot for the mapping function # create a line plot for the mapping function plt.plot(x_line, y_line, label=&#39;Polynomial&#39;, color=&#39;purple&#39;, alpha=1, linewidth=1.2, linestyle=&#39;dashed&#39;) plt.title(&#39;Using a polynomial model to approximate data&#39;, fontsize=12) plt.xlabel(&#39;&#39;, fontsize=10) plt.ylabel(&#39;&#39;, fontsize=10) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(True) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.tick_params( axis=&#39;x&#39;, # changes apply to the x-axis which=&#39;both&#39;, # both major and minor ticks are affected bottom=False, # ticks along the bottom edge are off top=False, # ticks along the top edge are off labelbottom=True) # labels along the bottom edge are off plt.tick_params( axis=&#39;y&#39;, # changes apply to the y-axis which=&#39;both&#39;, # both major and minor ticks are affected left=False, # ticks along the bottom edge are off right=False, # ticks along the top edge are off labelleft=True) # labels along the bottom edge are off plt.grid(False) plt.gca().yaxis.grid(True) plt.legend(fancybox=False, framealpha=1, shadow=False, borderpad=1) plt.savefig(&#39;linear_model_chart.png&#39;,dpi=300,bbox_inches=&#39;tight&#39;) plt.show() 5.4 — "],["time-series.html", "Chapter 6 Time series 6.1 Australian economic data 6.2 Seasonally adjusted data", " Chapter 6 Time series Almost all economic, finance, and business related data sets are measured over time (e.g. revenues, profits, margins, stock prices etc). Therefore the ability to both work with - and manipulate - dates and times becomes critical in using python for analysis. The book Coding for Economists has a wonderful explanation of the basics behind time series, time zones, and creating a date object from a string input. import matplotlib.pyplot as plt import matplotlib.dates as mdates import pandas as pd import numpy as np import statistics from scipy.stats import norm from matplotlib.ticker import EngFormatter, StrMethodFormatter gdp = pd.read_csv(&#39;data/gdp_1960_2020.csv&#39;) # The GDP numbers here are very long. To make them easier to read we can divide the GDP number by 1 billion. gdp[&#39;gdp_billions&#39;] = gdp[&#39;gdp&#39;] / 1_000_000_000 # Convert the year to datetime gdp[&#39;date&#39;] = pd.to_datetime(gdp[&#39;year&#39;], format=&#39;%Y&#39;) # Filter for Australia aus_gdp = gdp[gdp.country == &quot;Australia&quot;] aus_gdp.tail() Now that we’ve imported, filtered, and set up the data, let’s put it on a chart. # Setup plot size. fig, ax = plt.subplots(figsize=(8,4)) # Create grid # Zorder tells it which layer to put it on. We are setting this to 1 and our data to 2 so the grid is behind the data. ax.grid(which=&quot;major&quot;, axis=&#39;y&#39;, color=&#39;#758D99&#39;, alpha=0.6, zorder=1) # Plot data ax.plot(aus_gdp[&#39;date&#39;],aus_gdp[&#39;gdp_billions&#39;], color=&#39;#006BA2&#39;, linewidth=2) # Remove splines. Can be done one at a time or can slice with a list. ax.spines[[&#39;top&#39;,&#39;right&#39;,&#39;left&#39;]].set_visible(False) # Shrink y-lim to make plot a bit tigheter ax.set_ylim(0, 1950) # Set xlim to fit data without going over plot area ax.set_xlim(pd.datetime(1960, 1, 1), pd.datetime(2020, 1, 1)) # Reformat x-axis tick labels ax.xaxis.set_tick_params(labelsize=11) # Set tick label size # Reformat y-axis tick labels ax.set_yticklabels(np.arange(0,2000,250), # Set labels again ha = &#39;left&#39;, # Set horizontal alignment to right verticalalignment=&#39;bottom&#39;) # Set vertical alignment to make labels on top of gridline ax.yaxis.set_tick_params(pad=2, # Pad tick labels so they don&#39;t go over y-axis labeltop=True, # Put x-axis labels on top labelbottom=False, # Set no x-axis labels on bottom bottom=False, # Set no ticks on bottom labelsize=11) # Set tick label size #ax.yaxis.set_label_position(&quot;left&quot;) ax.yaxis.tick_left() ax.yaxis.set_major_formatter(&#39;${x:1.0f}bn&#39;) # Add in line and tag ax.plot([0.12, .9], # Set width of line [.98, .98], # Set height of line transform=fig.transFigure, # Set location relative to plot clip_on=False, color=&#39;#E3120B&#39;, linewidth=.6) # Add in title and subtitle ax.text(x=0.12, y=.93, s=&quot;Up, up, and away&quot;, transform=fig.transFigure, ha=&#39;left&#39;, fontsize=13, weight=&#39;bold&#39;, alpha=.8) ax.text(x=0.12, y=.88, s=&quot;Australia&#39;s GDP growth, in billions of USD, 1960-2020&quot;, transform=fig.transFigure, ha=&#39;left&#39;, fontsize=11, alpha=.8) # Set source text ax.text(x=0.12, y=0.01, s=&quot;&quot;&quot;Source: Kaggle GDP data (1960-2020)&quot;&quot;&quot;, transform=fig.transFigure, ha=&#39;left&#39;, fontsize=9, alpha=.7) # Export plot as high resolution PNG plt.savefig(&#39;docs/Aus_line.png&#39;, # Set path and filename dpi = 300, # Set dots per inch bbox_inches=&quot;tight&quot;, # Remove extra whitespace around plot facecolor=&#39;white&#39;) # Set background color to white plt.show() 6.1 Australian economic data Our central agencies (e.g. Treasury and RBA) certainly don’t make it easy to work with economic data. The easiest way (even in 2022) is to download poorly formatted csv’s. You can read a bit more about these methods here. 6.2 Seasonally adjusted data Chad Fulton has done a superb write up of the necessity to adjust for seasons (including outliers like Christmas Day) using the New York City COVID-19 daily case number data set. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
